{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:23:33.065767Z",
     "start_time": "2021-01-26T07:23:32.363150Z"
    }
   },
   "outputs": [],
   "source": [
    "from ppo_lr_schedule import PPO, Memory, FClayer, WaveNET, plot, ActorCritic\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "import gym_Aircraft\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:23:35.730259Z",
     "start_time": "2021-01-26T07:23:33.126156Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-kunwoopark/.local/lib/python3.7/site-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.activation.LeakyReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "actor_model = torch.load(\"./Custom_model_fin\")\n",
    "critic_model = torch.load(\"./Custom_model_fin\")\n",
    "test_model = torch.load(\"./Custom_model_fin\")\n",
    "mean = np.load('mean_test.npy')\n",
    "std = np.load('std_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:23:35.735085Z",
     "start_time": "2021-01-26T07:23:35.731972Z"
    }
   },
   "outputs": [],
   "source": [
    "num_final_nodes = critic_model.fin_fc.in_features\n",
    "critic_model.fin_fc = nn.Linear(num_final_nodes, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:23:35.759899Z",
     "start_time": "2021-01-26T07:23:35.736432Z"
    }
   },
   "outputs": [],
   "source": [
    "actor_model.fin_fc = nn.Sequential(actor_model.fin_fc, nn.Softmax(dim=-1))\n",
    "test_model.fin_fc = nn.Sequential(test_model.fin_fc, nn.Softmax(dim=-1))\n",
    "test_model=test_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:23:35.814983Z",
     "start_time": "2021-01-26T07:23:35.801574Z"
    }
   },
   "outputs": [],
   "source": [
    "# set angular constants\n",
    "Deg2Rad = np.pi/180\n",
    "Rad2Deg = 1/Deg2Rad\n",
    "\n",
    "############## Hyperparameters ##############\n",
    "succeed_coef = 100         # maximum reward when agent avoids collision\n",
    "collide_coef = -4000        # reward when agent doesn't avoid collision\n",
    "change_cmd_penalty = -100   # reward when agent changes command values\n",
    "cmd_penalty = -0.13          # coefficient of penaly on using command\n",
    "cmd_suit_coef = -100         # coefficient of suitable command\n",
    "start_cond_coef = 0.5       # coefficient of condition on begining\n",
    "\n",
    "solved_reward = 7000       # stop training if avg_reward > solved_reward\n",
    "log_interval = 50          # print avg reward in the interval\n",
    "max_episodes = 50000      # max training episodes\n",
    "max_timesteps = 300         # max timesteps in one episode\n",
    "n_latent_var = 60           # number of variables in hidden layer\n",
    "update_timestep = 2000      # update policy every n timesteps\n",
    "lr = 0.0001\n",
    "betas = (0.9, 0.999)\n",
    "gamma = 0.999                # discount factor\n",
    "K_epochs = 4                # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "step_size = 10000          # lr scheduling step size\n",
    "random_seed = 1\n",
    "\n",
    "# creating environment\n",
    "experiment_version = 1.28\n",
    "env_name = \"acav-v0\"\n",
    "env = gym.make(env_name)\n",
    "test_env = gym.make(env_name)\n",
    "env.env.__init__(succeed_coef, collide_coef, change_cmd_penalty, cmd_penalty, start_cond_coef, cmd_suit_coef)\n",
    "render = False\n",
    "\n",
    "#############################################\n",
    "\n",
    "if random_seed:\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "\n",
    "memory = Memory()\n",
    "ppo = PPO(actor_model, critic_model, lr, betas, gamma, K_epochs, eps_clip, step_size)\n",
    "\n",
    "# logging variables\n",
    "running_reward = 0\n",
    "test_running_reward = 0\n",
    "avg_length = 0\n",
    "timestep = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:02:30.503732Z",
     "start_time": "2021-01-26T07:23:35.816674Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize lists for print\n",
    "rewards = []\n",
    "total_res=[]\n",
    "test_rewards = []\n",
    "test_total_res=[]\n",
    "\n",
    "# training loop\n",
    "for i_episode in range(1, max_episodes+1):\n",
    "    epi_reward = 0\n",
    "    test_epi_reward = 0\n",
    "    \n",
    "    state = env.reset()\n",
    "    test_env = copy.deepcopy(env)\n",
    "\n",
    "    res_list = np.zeros(12)\n",
    "    test_res_list = np.zeros(12)\n",
    "    test_done = False\n",
    "    for t in range(max_timesteps):\n",
    "        timestep += 1\n",
    "        state = (state-mean) / std\n",
    "        \n",
    "        # Test with the initial model\n",
    "        with torch.no_grad():\n",
    "            if not test_done:\n",
    "                if t == 0:\n",
    "                    test_state = state\n",
    "                test_action = test_model(torch.from_numpy(test_state).float().to(device)).max(0)[1].item()\n",
    "                test_state, test_reward, test_done, test_info = test_env.step(test_action)\n",
    "                test_state = (test_state - mean) / std\n",
    "                cmd_list, r_list, elev_list, azim_list, Pm_list, Pt_list, h_list, height_diff_list = test_info[\"info\"]\n",
    "                Pm_list = Pm_list.tolist()\n",
    "                Pt_list = Pt_list.tolist()\n",
    "                merged_data = itertools.chain([cmd_list], [r_list], [elev_list], [azim_list], Pm_list, Pt_list, [h_list], [height_diff_list])\n",
    "                merged_data = np.array(list(merged_data))\n",
    "                test_res_list = np.vstack([test_res_list,merged_data])\n",
    "                test_epi_reward += test_reward\n",
    "                if test_done:\n",
    "                    test_running_reward += test_epi_reward\n",
    "                    test_res_list=np.delete(test_res_list,0,0)\n",
    "                    test_total_res.append(test_res_list)\n",
    "                \n",
    "        \n",
    "        action = ppo.policy_old.act(state, memory)   \n",
    "            \n",
    "        # Running policy_old:    \n",
    "        state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # save data to print\n",
    "        cmd_list, r_list, elev_list, azim_list, Pm_list, Pt_list, h_list, height_diff_list=info[\"info\"]\n",
    "        Pm_list = Pm_list.tolist()\n",
    "        Pt_list = Pt_list.tolist()\n",
    "        merged_data = itertools.chain([cmd_list], [r_list], [elev_list], [azim_list], Pm_list, Pt_list, [h_list], [height_diff_list])\n",
    "        merged_data = np.array(list(merged_data))\n",
    "        res_list = np.vstack([res_list,merged_data])\n",
    "\n",
    "        # Saving reward and is_terminal:\n",
    "        memory.rewards.append(reward)\n",
    "        memory.is_terminals.append(done)\n",
    "\n",
    "        # update if its time\n",
    "        if timestep % update_timestep == 0:\n",
    "            ppo.update(memory)\n",
    "            memory.clear_memory()\n",
    "            timestep = 0\n",
    "\n",
    "        running_reward += reward\n",
    "        epi_reward += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            res_list=np.delete(res_list,0,0)\n",
    "            total_res.append(res_list)\n",
    "            if not test_done:\n",
    "                test_running_reward += test_epi_reward\n",
    "                test_res_list = np.delete(test_res_list,0,0)\n",
    "                test_total_res.append(test_res_list)\n",
    "            break\n",
    "\n",
    "    avg_length += t\n",
    "    \n",
    "    # lr sheduler step\n",
    "    ppo.scheduler.step()\n",
    "\n",
    "    # stop training if avg_reward > solved_reward\n",
    "    if running_reward > (log_interval*solved_reward):\n",
    "        print(\"########## Solved! ##########\")\n",
    "        torch.save(ppo.policy.state_dict(), './PPO_{}.pth'.format(experiment_version))\n",
    "        break\n",
    "\n",
    "    # logging\n",
    "    if i_episode % log_interval == 0:\n",
    "        avg_length = avg_length / log_interval\n",
    "        running_reward = running_reward / log_interval\n",
    "        test_running_reward = test_running_reward / log_interval\n",
    "        rewards.append(running_reward)\n",
    "        test_rewards.append(test_running_reward)\n",
    "        plot(i_episode, rewards, total_res, test_rewards, test_total_res)\n",
    "\n",
    "        print('Episode {} | avg length: {} | run_reward: {} | min_r: {:.2f} | reward: {} | init_height_diff: {:.2f} \\n \\t \\\n",
    "              test_run_reward: {} | test_min_r: {:.2f} | test_reward: {}'\\\n",
    "              .format(i_episode, avg_length, running_reward,min(total_res[-1][:,1]),\\\n",
    "                      epi_reward, total_res[-1][0,-1], test_running_reward,min(test_total_res[-1][:,1]),test_epi_reward), end=\"\\r\")\n",
    "        running_reward = 0\n",
    "        test_running_reward = 0\n",
    "        avg_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ppo.policy.state_dict(), './PPO_{}.pth'.format(experiment_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:02:30.944985Z",
     "start_time": "2021-01-26T11:02:30.505226Z"
    }
   },
   "outputs": [],
   "source": [
    "Deg2Rad = np.pi/180\n",
    "Rad2Deg = 1/Deg2Rad\n",
    "\n",
    "plt_res=total_res[-1]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,9), dpi=100)\n",
    "\n",
    "plt.subplot(511)\n",
    "plt.plot(plt_res[:,0], label=r'$\\dot{h}_{cmd}$')\n",
    "plt.ylabel(r'$\\dot{h}_{cmd}$ ($m/s$)'), plt.grid()\n",
    "\n",
    "plt.subplot(512)\n",
    "plt.plot(plt_res[:,10],label=r'$\\{h}$')\n",
    "plt.ylabel(r'$h$ (m)'), plt.grid()\n",
    "\n",
    "plt.subplot(513)\n",
    "plt.plot(plt_res[:,1],label=r'$\\{r}$')\n",
    "plt.ylabel(r'$r$ (m)'), plt.grid()\n",
    "\n",
    "plt.subplot(514)\n",
    "plt.plot(plt_res[:,2]*Rad2Deg, label='elevation')\n",
    "plt.ylabel('elevation (deg)'), plt.grid()\n",
    "\n",
    "plt.subplot(515)\n",
    "plt.plot(plt_res[:,3]*Rad2Deg, label='azimuth')\n",
    "plt.ylabel('azimuth (deg)'), plt.grid()\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:02:31.378415Z",
     "start_time": "2021-01-26T11:02:30.946424Z"
    }
   },
   "outputs": [],
   "source": [
    "# trajectory plots\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.figure(figsize=(12,9), dpi=100)\n",
    "plt.gca(projection='3d')\n",
    "plt.plot(plt_res[:,5], plt_res[:,4], -plt_res[:,6], label='player', linewidth=3)\n",
    "plt.plot(plt_res[:,8], plt_res[:,7], -plt_res[:,9], label='target', linewidth=3)\n",
    "plt.xlabel('East')\n",
    "plt.ylabel('North')\n",
    "plt.xlim(-2000,2000)\n",
    "plt.ylim(0,4000)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,9), dpi=100)\n",
    "plt.plot(plt_res[:,5], plt_res[:,4], label='player', linewidth=3)\n",
    "plt.plot(plt_res[:,8], plt_res[:,7], label='target', linewidth=3)\n",
    "plt.xlabel('East')\n",
    "plt.ylabel('North')\n",
    "plt.grid(), plt.legend(), plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,9), dpi=100)\n",
    "plt.plot(plt_res[:,4], -plt_res[:,6], label='player', linewidth=3)\n",
    "plt.plot(plt_res[:,7], -plt_res[:,9], label='target', linewidth=3)\n",
    "plt.xlabel('North')\n",
    "plt.ylabel('Up')\n",
    "plt.grid(), plt.legend(), plt.axis('equal')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
