{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:23:33.065767Z",
     "start_time": "2021-01-26T07:23:32.363150Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.distributions import Categorical\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "import gym\n",
    "import gym_Aircraft\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:23:33.071309Z",
     "start_time": "2021-01-26T07:23:33.067881Z"
    }
   },
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:23:33.122796Z",
     "start_time": "2021-01-26T07:23:33.072763Z"
    }
   },
   "outputs": [],
   "source": [
    "class FClayer(nn.Module):\n",
    "    def __init__(self, innodes: int, nodes: int):\n",
    "        super(FClayer, self).__init__()\n",
    "        self.fc=nn.Linear(innodes,nodes)\n",
    "        self.act=nn.LeakyReLU(0.2, inplace=True)\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        out=self.fc(x)\n",
    "        out=self.act(out)\n",
    "        return out\n",
    "class WaveNET(nn.Module):\n",
    "    def __init__(self, block: Type[Union[FClayer]], planes: List[int], nodes: List[int], num_classes: int = 3\n",
    "                ) -> None:\n",
    "        super(WaveNET, self).__init__()\n",
    "        self.innodes=5\n",
    "        \n",
    "        self.layer1=self._make_layer(block, planes[0], nodes[0])\n",
    "        self.layer2=self._make_layer(block, planes[1], nodes[1])\n",
    "        self.layer3=self._make_layer(block, planes[2], nodes[2])\n",
    "        \n",
    "        self.fin_fc=nn.Linear(self.innodes,num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "    \n",
    "    def _make_layer(self, block: Type[Union[FClayer]], planes: int, nodes: int) -> nn.Sequential:\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.innodes, nodes))\n",
    "        self.innodes = nodes\n",
    "        for _ in range(1, planes):\n",
    "            layers.append(block(self.innodes, nodes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "        \n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.fin_fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:23:35.730259Z",
     "start_time": "2021-01-26T07:23:33.126156Z"
    }
   },
   "outputs": [],
   "source": [
    "actor_model = torch.load(\"./Custom_model_fin\")\n",
    "critic_model = torch.load(\"./Custom_model_fin\")\n",
    "mean = np.load('mean_test.npy')\n",
    "std = np.load('std_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:23:35.735085Z",
     "start_time": "2021-01-26T07:23:35.731972Z"
    }
   },
   "outputs": [],
   "source": [
    "num_final_nodes = critic_model.fin_fc.in_features\n",
    "critic_model.fin_fc = nn.Linear(num_final_nodes, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:23:35.759899Z",
     "start_time": "2021-01-26T07:23:35.736432Z"
    }
   },
   "outputs": [],
   "source": [
    "num_final_nodes = actor_model.fin_fc.in_features\n",
    "num_output_nodes = actor_model.fin_fc.out_features\n",
    "actor_model.fin_fc = nn.Sequential(nn.Linear(num_final_nodes, num_output_nodes), nn.Softmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:23:35.783593Z",
     "start_time": "2021-01-26T07:23:35.762038Z"
    }
   },
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, actor_model, critic_model):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        # actor\n",
    "        self.action_layer = actor_model\n",
    "        \n",
    "        # critic\n",
    "        self.value_layer = critic_model\n",
    "        \n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def act(self, state, memory):\n",
    "        state = torch.from_numpy(state).float().to(device) \n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(action)\n",
    "        memory.logprobs.append(dist.log_prob(action))\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def evaluate(self, state, action):\n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        \n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        \n",
    "        state_value = self.value_layer(state)\n",
    "        \n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy\n",
    "        \n",
    "class PPO:\n",
    "    def __init__(self, actor_model, critic_model, lr, betas, gamma, K_epochs, eps_clip):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.policy = ActorCritic(actor_model, critic_model).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
    "        self.policy_old = ActorCritic(actor_model, critic_model).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "    \n",
    "    def update(self, memory):   \n",
    "        # Monte Carlo estimate of state rewards:\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "        \n",
    "        # Normalizing the rewards:\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "        \n",
    "        # convert list to tensor\n",
    "        old_states = torch.stack(memory.states).to(device).detach()\n",
    "        old_actions = torch.stack(memory.actions).to(device).detach()\n",
    "        old_logprobs = torch.stack(memory.logprobs).to(device).detach()\n",
    "        \n",
    "        # Optimize policy for K epochs:\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluating old actions and values :\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "            \n",
    "            # Finding the ratio (pi_theta / pi_theta__old):\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "                \n",
    "            # Finding Surrogate Loss:\n",
    "            advantages = rewards - state_values.detach()\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "            \n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        # Copy new weights into old policy:\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:23:35.799718Z",
     "start_time": "2021-01-26T07:23:35.785843Z"
    }
   },
   "outputs": [],
   "source": [
    "Deg2Rad = np.pi/180\n",
    "Rad2Deg = 1/Deg2Rad\n",
    "def plot(epi_idx, rewards, total_res):\n",
    "    plt_res=total_res[-1]\n",
    "    \n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,15))\n",
    "    plt.subplot(411)\n",
    "    plt.title('Episodes %s. reward: %s' % (epi_idx, rewards[-1]))\n",
    "    plt.plot(rewards),plt.grid()\n",
    "    \n",
    "    plt.subplot(412)\n",
    "    plt.plot(plt_res[:,0], label=r'$\\dot{h}_{cmd}$')\n",
    "    plt.ylabel(r'$\\dot{h}_{cmd}$ ($m/s$)'), plt.grid()\n",
    "    plt.subplot(413)\n",
    "    plt.plot(plt_res[:,10],label=r'$\\{h}$')\n",
    "    plt.ylabel(r'$h$ (m)'), plt.grid()\n",
    "    plt.subplot(414)\n",
    "    plt.plot(plt_res[:,1],label=r'$\\{r}$')\n",
    "    plt.ylabel(r'$r$ (m)'), plt.grid()\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:23:35.814983Z",
     "start_time": "2021-01-26T07:23:35.801574Z"
    }
   },
   "outputs": [],
   "source": [
    "############## Hyperparameters ##############\n",
    "succeed_coef = 10000        # maximum reward when agent avoids collision\n",
    "change_cmd_penalty = -100   # reward when agent changes command values\n",
    "cmd_penalty = -0.06         # coefficient of penaly on using command\n",
    "start_cond_coef = 0.4       # coefficient of condition on begining\n",
    "solved_reward = 10000       # stop training if avg_reward > solved_reward\n",
    "log_interval = 100          # print avg reward in the interval\n",
    "max_episodes = 100000       # max training episodes\n",
    "max_timesteps = 300         # max timesteps in one episode\n",
    "n_latent_var = 60           # number of variables in hidden layer\n",
    "update_timestep = 2000      # update policy every n timesteps\n",
    "lr = 0.002\n",
    "betas = (0.9, 0.999)\n",
    "gamma = 0.99                # discount factor\n",
    "K_epochs = 4                # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "action_maintain_steps = 1    # steps to maintain the action\n",
    "random_seed = None\n",
    "\n",
    "# creating environment\n",
    "experiment_version = 1.22\n",
    "env_name = \"acav-v0\"\n",
    "env = gym.make(env_name)\n",
    "env.env.__init__(succeed_coef, change_cmd_penalty, cmd_penalty, start_cond_coef)\n",
    "render = False\n",
    "\n",
    "#############################################\n",
    "\n",
    "if random_seed:\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "\n",
    "memory = Memory()\n",
    "ppo = PPO(actor_model, critic_model, lr, betas, gamma, K_epochs, eps_clip)\n",
    "# load weight to transfer knowledges\n",
    "# ppo.policy_old.load_state_dict(torch.load(\"PPO_1.2.pth\"))\n",
    "\n",
    "# logging variables\n",
    "running_reward = 0\n",
    "avg_length = 0\n",
    "timestep = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:02:30.503732Z",
     "start_time": "2021-01-26T07:23:35.816674Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize lists for print\n",
    "rewards   = []\n",
    "total_res=[]\n",
    "\n",
    "# training loop\n",
    "for i_episode in range(1, max_episodes+1):\n",
    "    epi_reward=0\n",
    "    action_steps = action_maintain_steps\n",
    "    \n",
    "    state = env.reset()\n",
    "\n",
    "    res_list=np.zeros(11)\n",
    "    \n",
    "    for t in range(max_timesteps):\n",
    "        timestep += 1\n",
    "        state = (state-mean)/std\n",
    "\n",
    "        # make action maintained\n",
    "        if action_steps == action_maintain_steps:\n",
    "            action = ppo.policy_old.act(state, memory)\n",
    "            action_steps = 0\n",
    "        else:\n",
    "            ppo.policy_old.act(state, memory)\n",
    "            \n",
    "        # Running policy_old:    \n",
    "        state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # count number of action steps\n",
    "        action_steps += 1\n",
    "        \n",
    "        # save data to print\n",
    "        cmd_list,r_list,elev_list,azim_list,Pm_list,Pt_list,h_list=info[\"info\"]\n",
    "        Pm_list=Pm_list.tolist()\n",
    "        Pt_list=Pt_list.tolist()\n",
    "        merged_data=itertools.chain([cmd_list],[r_list],[elev_list],[azim_list],Pm_list,Pt_list,[h_list])\n",
    "        merged_data=np.array(list(merged_data))\n",
    "        res_list=np.vstack([res_list,merged_data])\n",
    "\n",
    "        # Saving reward and is_terminal:\n",
    "        memory.rewards.append(reward)\n",
    "        memory.is_terminals.append(done)\n",
    "\n",
    "        # update if its time\n",
    "        if timestep % update_timestep == 0:\n",
    "            ppo.update(memory)\n",
    "            memory.clear_memory()\n",
    "            timestep = 0\n",
    "\n",
    "        running_reward += reward\n",
    "        epi_reward += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            res_list=np.delete(res_list,0,0)\n",
    "            total_res.append(res_list)\n",
    "            break\n",
    "\n",
    "    avg_length += t\n",
    "\n",
    "    # stop training if avg_reward > solved_reward\n",
    "    if running_reward > (log_interval*solved_reward):\n",
    "        print(\"########## Solved! ##########\")\n",
    "        torch.save(ppo.policy.state_dict(), './PPO_{}.pth'.format(experiment_version))\n",
    "        break\n",
    "\n",
    "    # logging\n",
    "    if i_episode % log_interval == 0:\n",
    "        avg_length = avg_length/log_interval\n",
    "        running_reward = running_reward/log_interval\n",
    "        rewards.append(running_reward)\n",
    "        plot(i_episode,rewards,total_res)\n",
    "\n",
    "        print('Episode {} \\t avg length: {} \\t run_reward: {} \\t min_r: {} \\t reward: {}'.format(i_episode, avg_length, running_reward,min(total_res[-1][:,1]),epi_reward), end=\"\\r\")\n",
    "        running_reward = 0\n",
    "        avg_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ppo.policy.state_dict(), './PPO_{}.pth'.format(experiment_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:02:30.944985Z",
     "start_time": "2021-01-26T11:02:30.505226Z"
    }
   },
   "outputs": [],
   "source": [
    "Deg2Rad = np.pi/180\n",
    "Rad2Deg = 1/Deg2Rad\n",
    "\n",
    "plt_res=total_res[-1]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,9), dpi=100)\n",
    "\n",
    "plt.subplot(511)\n",
    "plt.plot(plt_res[:,0], label=r'$\\dot{h}_{cmd}$')\n",
    "plt.ylabel(r'$\\dot{h}_{cmd}$ ($m/s$)'), plt.grid()\n",
    "\n",
    "plt.subplot(512)\n",
    "plt.plot(plt_res[:,10],label=r'$\\{h}$')\n",
    "plt.ylabel(r'$h$ (m)'), plt.grid()\n",
    "\n",
    "plt.subplot(513)\n",
    "plt.plot(plt_res[:,1],label=r'$\\{r}$')\n",
    "plt.ylabel(r'$r$ (m)'), plt.grid()\n",
    "\n",
    "plt.subplot(514)\n",
    "plt.plot(plt_res[:,2]*Rad2Deg, label='elevation')\n",
    "plt.ylabel('elevation (deg)'), plt.grid()\n",
    "\n",
    "plt.subplot(515)\n",
    "plt.plot(plt_res[:,3]*Rad2Deg, label='azimuth')\n",
    "plt.ylabel('azimuth (deg)'), plt.grid()\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:02:31.378415Z",
     "start_time": "2021-01-26T11:02:30.946424Z"
    }
   },
   "outputs": [],
   "source": [
    "# trajectory plots\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.figure(figsize=(12,9), dpi=100)\n",
    "plt.gca(projection='3d')\n",
    "plt.plot(plt_res[:,5], plt_res[:,4], -plt_res[:,6], label='player', linewidth=3)\n",
    "plt.plot(plt_res[:,8], plt_res[:,7], -plt_res[:,9], label='target', linewidth=3)\n",
    "plt.xlabel('East')\n",
    "plt.ylabel('North')\n",
    "plt.xlim(-2000,2000)\n",
    "plt.ylim(0,4000)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,9), dpi=100)\n",
    "plt.plot(plt_res[:,5], plt_res[:,4], label='player', linewidth=3)\n",
    "plt.plot(plt_res[:,8], plt_res[:,7], label='target', linewidth=3)\n",
    "plt.xlabel('East')\n",
    "plt.ylabel('North')\n",
    "plt.grid(), plt.legend(), plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,9), dpi=100)\n",
    "plt.plot(plt_res[:,4], -plt_res[:,6], label='player', linewidth=3)\n",
    "plt.plot(plt_res[:,7], -plt_res[:,9], label='target', linewidth=3)\n",
    "plt.xlabel('North')\n",
    "plt.ylabel('Up')\n",
    "plt.grid(), plt.legend(), plt.axis('equal')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
