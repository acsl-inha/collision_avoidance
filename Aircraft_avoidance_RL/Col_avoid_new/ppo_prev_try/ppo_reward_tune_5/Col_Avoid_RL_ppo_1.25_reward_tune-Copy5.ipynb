{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:23:33.065767Z",
     "start_time": "2021-01-26T07:23:32.363150Z"
    }
   },
   "outputs": [],
   "source": [
    "from ppo2 import PPO, Memory, FClayer, WaveNET, plot, ActorCritic\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "import gym_Aircraft\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:23:35.730259Z",
     "start_time": "2021-01-26T07:23:33.126156Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-kunwoopark/.local/lib/python3.7/site-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/jupyter-kunwoopark/.local/lib/python3.7/site-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/jupyter-kunwoopark/.local/lib/python3.7/site-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.activation.LeakyReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "actor_model = torch.load(\"./Custom_model_fin\")\n",
    "critic_model = torch.load(\"./Custom_model_fin\")\n",
    "mean = np.load('mean_test.npy')\n",
    "std = np.load('std_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:23:35.735085Z",
     "start_time": "2021-01-26T07:23:35.731972Z"
    }
   },
   "outputs": [],
   "source": [
    "num_final_nodes = critic_model.fin_fc.in_features\n",
    "critic_model.fin_fc = nn.Linear(num_final_nodes, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:23:35.759899Z",
     "start_time": "2021-01-26T07:23:35.736432Z"
    }
   },
   "outputs": [],
   "source": [
    "num_final_nodes = actor_model.fin_fc.in_features\n",
    "num_output_nodes = actor_model.fin_fc.out_features\n",
    "actor_model.fin_fc = nn.Sequential(nn.Linear(num_final_nodes, num_output_nodes), nn.Softmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:23:35.814983Z",
     "start_time": "2021-01-26T07:23:35.801574Z"
    }
   },
   "outputs": [],
   "source": [
    "# set angular constants\n",
    "Deg2Rad = np.pi/180\n",
    "Rad2Deg = 1/Deg2Rad\n",
    "\n",
    "############## Hyperparameters ##############\n",
    "succeed_coef = 100         # maximum reward when agent avoids collision\n",
    "collide_coef = -4000        # reward when agent doesn't avoid collision\n",
    "change_cmd_penalty = -100   # reward when agent changes command values\n",
    "cmd_penalty = -0.05          # coefficient of penaly on using command\n",
    "cmd_suit_coef = -100         # coefficient of suitable command\n",
    "start_cond_coef = 0.5       # coefficient of condition on begining\n",
    "\n",
    "solved_reward = 100000       # stop training if avg_reward > solved_reward\n",
    "log_interval = 50          # print avg reward in the interval\n",
    "max_episodes = 1000000      # max training episodes\n",
    "max_timesteps = 300         # max timesteps in one episode\n",
    "n_latent_var = 60           # number of variables in hidden layer\n",
    "update_timestep = 2000      # update policy every n timesteps\n",
    "lr = 0.002\n",
    "betas = (0.9, 0.999)\n",
    "gamma = 0.999                # discount factor\n",
    "K_epochs = 4                # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "action_maintain_steps = 1    # steps to maintain the action\n",
    "\n",
    "# initialize epsilon for greedy algorithm at start\n",
    "e_start = 1.0               \n",
    "e_final = 0.01\n",
    "e_decay = 500\n",
    "epsilon_by_frame = lambda frame_idx: e_final + (e_start - e_final) * math.exp(-1. * frame_idx / e_decay)\n",
    "random_seed = None\n",
    "\n",
    "# creating environment\n",
    "experiment_version = 1.24\n",
    "env_name = \"acav-v0\"\n",
    "env = gym.make(env_name)\n",
    "env.env.__init__(succeed_coef, collide_coef, change_cmd_penalty, cmd_penalty, start_cond_coef, cmd_suit_coef)\n",
    "render = False\n",
    "\n",
    "#############################################\n",
    "\n",
    "if random_seed:\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "\n",
    "memory = Memory()\n",
    "ppo = PPO(actor_model, critic_model, lr, betas, gamma, K_epochs, eps_clip)\n",
    "# load weight to transfer knowledges\n",
    "# ppo.policy_old.load_state_dict(torch.load(\"PPO_1.2.pth\"))\n",
    "\n",
    "# logging variables\n",
    "running_reward = 0\n",
    "avg_length = 0\n",
    "timestep = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:02:30.503732Z",
     "start_time": "2021-01-26T07:23:35.816674Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-41200fd4135d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# make action maintained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maction_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0maction_maintain_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_old\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_by_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0maction_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hdd1/nfs/kunwoopark/Research/collision_avoidance/Aircraft_avoidance_RL/Col_avoid_new/ppo_reward_tune_5/ppo2.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state, memory, e_greedy)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0me_greedy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# initialize lists for print\n",
    "rewards   = []\n",
    "total_res=[]\n",
    "\n",
    "# training loop\n",
    "for i_episode in range(1, max_episodes+1):\n",
    "    epi_reward=0\n",
    "    action_steps = action_maintain_steps\n",
    "    \n",
    "    state = env.reset()\n",
    "\n",
    "    res_list=np.zeros(11)\n",
    "    \n",
    "    for t in range(max_timesteps):\n",
    "        timestep += 1\n",
    "        state = (state-mean)/std\n",
    "\n",
    "        # make action maintained\n",
    "        if action_steps == action_maintain_steps:\n",
    "            action = ppo.policy_old.act(state, memory, epsilon_by_frame(i_episode))\n",
    "            action_steps = 0\n",
    "        else:\n",
    "            ppo.policy_old.act(state, memory, epsilon_by_frame(i_episode))\n",
    "            \n",
    "        # Running policy_old:    \n",
    "        state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # count number of action steps\n",
    "        action_steps += 1\n",
    "        \n",
    "        # save data to print\n",
    "        cmd_list,r_list,elev_list,azim_list,Pm_list,Pt_list,h_list=info[\"info\"]\n",
    "        Pm_list=Pm_list.tolist()\n",
    "        Pt_list=Pt_list.tolist()\n",
    "        merged_data=itertools.chain([cmd_list],[r_list],[elev_list],[azim_list],Pm_list,Pt_list,[h_list])\n",
    "        merged_data=np.array(list(merged_data))\n",
    "        res_list=np.vstack([res_list,merged_data])\n",
    "\n",
    "        # Saving reward and is_terminal:\n",
    "        memory.rewards.append(reward)\n",
    "        memory.is_terminals.append(done)\n",
    "\n",
    "        # update if its time\n",
    "        if timestep % update_timestep == 0:\n",
    "            ppo.update(memory)\n",
    "            memory.clear_memory()\n",
    "            timestep = 0\n",
    "\n",
    "        running_reward += reward\n",
    "        epi_reward += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            res_list=np.delete(res_list,0,0)\n",
    "            total_res.append(res_list)\n",
    "            break\n",
    "\n",
    "    avg_length += t\n",
    "\n",
    "    # stop training if avg_reward > solved_reward\n",
    "    if running_reward > (log_interval*solved_reward):\n",
    "        print(\"########## Solved! ##########\")\n",
    "        torch.save(ppo.policy.state_dict(), './PPO_{}.pth'.format(experiment_version))\n",
    "        break\n",
    "\n",
    "    # logging\n",
    "    if i_episode % log_interval == 0:\n",
    "        avg_length = avg_length/log_interval\n",
    "        running_reward = running_reward/log_interval\n",
    "        rewards.append(running_reward)\n",
    "        plot(i_episode,rewards,total_res)\n",
    "\n",
    "        print('Episode {} \\t avg length: {} \\t run_reward: {} \\t min_r: {} \\t reward: {}'.format(i_episode, avg_length, running_reward,min(total_res[-1][:,1]),epi_reward), end=\"\\r\")\n",
    "        running_reward = 0\n",
    "        avg_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ppo.policy.state_dict(), './PPO_{}.pth'.format(experiment_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:02:30.944985Z",
     "start_time": "2021-01-26T11:02:30.505226Z"
    }
   },
   "outputs": [],
   "source": [
    "Deg2Rad = np.pi/180\n",
    "Rad2Deg = 1/Deg2Rad\n",
    "\n",
    "plt_res=total_res[-1]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,9), dpi=100)\n",
    "\n",
    "plt.subplot(511)\n",
    "plt.plot(plt_res[:,0], label=r'$\\dot{h}_{cmd}$')\n",
    "plt.ylabel(r'$\\dot{h}_{cmd}$ ($m/s$)'), plt.grid()\n",
    "\n",
    "plt.subplot(512)\n",
    "plt.plot(plt_res[:,10],label=r'$\\{h}$')\n",
    "plt.ylabel(r'$h$ (m)'), plt.grid()\n",
    "\n",
    "plt.subplot(513)\n",
    "plt.plot(plt_res[:,1],label=r'$\\{r}$')\n",
    "plt.ylabel(r'$r$ (m)'), plt.grid()\n",
    "\n",
    "plt.subplot(514)\n",
    "plt.plot(plt_res[:,2]*Rad2Deg, label='elevation')\n",
    "plt.ylabel('elevation (deg)'), plt.grid()\n",
    "\n",
    "plt.subplot(515)\n",
    "plt.plot(plt_res[:,3]*Rad2Deg, label='azimuth')\n",
    "plt.ylabel('azimuth (deg)'), plt.grid()\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T11:02:31.378415Z",
     "start_time": "2021-01-26T11:02:30.946424Z"
    }
   },
   "outputs": [],
   "source": [
    "# trajectory plots\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.figure(figsize=(12,9), dpi=100)\n",
    "plt.gca(projection='3d')\n",
    "plt.plot(plt_res[:,5], plt_res[:,4], -plt_res[:,6], label='player', linewidth=3)\n",
    "plt.plot(plt_res[:,8], plt_res[:,7], -plt_res[:,9], label='target', linewidth=3)\n",
    "plt.xlabel('East')\n",
    "plt.ylabel('North')\n",
    "plt.xlim(-2000,2000)\n",
    "plt.ylim(0,4000)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,9), dpi=100)\n",
    "plt.plot(plt_res[:,5], plt_res[:,4], label='player', linewidth=3)\n",
    "plt.plot(plt_res[:,8], plt_res[:,7], label='target', linewidth=3)\n",
    "plt.xlabel('East')\n",
    "plt.ylabel('North')\n",
    "plt.grid(), plt.legend(), plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,9), dpi=100)\n",
    "plt.plot(plt_res[:,4], -plt_res[:,6], label='player', linewidth=3)\n",
    "plt.plot(plt_res[:,7], -plt_res[:,9], label='target', linewidth=3)\n",
    "plt.xlabel('North')\n",
    "plt.ylabel('Up')\n",
    "plt.grid(), plt.legend(), plt.axis('equal')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
