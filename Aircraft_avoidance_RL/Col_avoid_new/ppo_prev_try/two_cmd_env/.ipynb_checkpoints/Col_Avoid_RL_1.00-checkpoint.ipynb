{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import time\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "import gym_Aircraft\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-kunwoopark/.local/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"acav-v0\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"transition 저장\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(env.observation_space.shape[0], 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, env.action_space.n)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=40, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=40, out_features=40, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=40, out_features=40, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=40, out_features=40, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=40, out_features=40, bias=True)\n",
       "    (9): ReLU()\n",
       "    (10): Linear(in_features=40, out_features=40, bias=True)\n",
       "    (11): ReLU()\n",
       "    (12): Linear(in_features=40, out_features=40, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Linear(in_features=40, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "target_net = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max (1)은 각 행의 가장 큰 열 값을 반환합니다.\n",
    "            # 최대 결과의 두번째 열은 최대 요소의 주소값이므로,\n",
    "            # 기대 보상이 더 큰 행동을 선택할 수 있습니다.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # 기대 Q 값 계산\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Huber 손실 계산\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # 모델 최적화\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/11/25 03:19:30\n",
      "episode : 0 | final step : 100 | total reward : 36.5\n",
      "2020/11/25 03:19:31\n",
      "episode : 1 | final step : 100 | total reward : 20.0\n",
      "2020/11/25 03:19:31\n",
      "episode : 2 | final step : 101 | total reward : 21.125\n",
      "2020/11/25 03:19:32\n",
      "episode : 3 | final step : 99 | total reward : 0.05000000074505806\n",
      "2020/11/25 03:19:33\n",
      "episode : 4 | final step : 99 | total reward : 20.600000381469727\n",
      "2020/11/25 03:19:33\n",
      "episode : 5 | final step : 99 | total reward : -52.07500076293945\n",
      "2020/11/25 03:19:34\n",
      "episode : 6 | final step : 100 | total reward : -81.2249984741211\n",
      "2020/11/25 03:19:34\n",
      "episode : 7 | final step : 99 | total reward : 43.650001525878906\n",
      "2020/11/25 03:19:35\n",
      "episode : 8 | final step : 98 | total reward : -56.04999923706055\n",
      "2020/11/25 03:19:36\n",
      "episode : 9 | final step : 98 | total reward : -60.625\n",
      "2020/11/25 03:19:36\n",
      "episode : 10 | final step : 99 | total reward : 36.42499923706055\n",
      "2020/11/25 03:19:37\n",
      "episode : 11 | final step : 96 | total reward : 31.649999618530273\n",
      "2020/11/25 03:19:38\n",
      "episode : 12 | final step : 98 | total reward : -82.2249984741211\n",
      "2020/11/25 03:19:38\n",
      "episode : 13 | final step : 99 | total reward : 18.25\n",
      "2020/11/25 03:19:39\n",
      "episode : 14 | final step : 99 | total reward : 34.04999923706055\n",
      "2020/11/25 03:19:39\n",
      "episode : 15 | final step : 99 | total reward : 56.349998474121094\n",
      "2020/11/25 03:19:40\n",
      "episode : 16 | final step : 99 | total reward : -41.5\n",
      "2020/11/25 03:19:41\n",
      "episode : 17 | final step : 99 | total reward : -70.25\n",
      "2020/11/25 03:19:41\n",
      "episode : 18 | final step : 99 | total reward : 58.900001525878906\n",
      "2020/11/25 03:19:42\n",
      "episode : 19 | final step : 98 | total reward : -52.474998474121094\n",
      "2020/11/25 03:19:42\n",
      "episode : 20 | final step : 99 | total reward : -77.44999694824219\n",
      "2020/11/25 03:19:43\n",
      "episode : 21 | final step : 100 | total reward : 48.099998474121094\n",
      "2020/11/25 03:19:44\n",
      "episode : 22 | final step : 99 | total reward : 36.974998474121094\n",
      "2020/11/25 03:19:44\n",
      "episode : 23 | final step : 98 | total reward : -68.92500305175781\n",
      "2020/11/25 03:19:45\n",
      "episode : 24 | final step : 97 | total reward : 61.20000076293945\n",
      "2020/11/25 03:19:45\n",
      "episode : 25 | final step : 100 | total reward : 20.25\n",
      "2020/11/25 03:19:46\n",
      "episode : 26 | final step : 98 | total reward : -72.32499694824219\n",
      "2020/11/25 03:19:46\n",
      "episode : 27 | final step : 100 | total reward : -77.92500305175781\n",
      "2020/11/25 03:19:47\n",
      "episode : 28 | final step : 99 | total reward : -90.82499694824219\n",
      "2020/11/25 03:19:48\n",
      "episode : 29 | final step : 98 | total reward : -80.82499694824219\n",
      "2020/11/25 03:19:48\n",
      "episode : 30 | final step : 99 | total reward : -69.25\n",
      "2020/11/25 03:19:49\n",
      "episode : 31 | final step : 98 | total reward : -39.92499923706055\n",
      "2020/11/25 03:19:49\n",
      "episode : 32 | final step : 100 | total reward : 32.92499923706055\n",
      "2020/11/25 03:19:50\n",
      "episode : 33 | final step : 98 | total reward : -62.5\n",
      "2020/11/25 03:19:51\n",
      "episode : 34 | final step : 98 | total reward : -50.875\n",
      "2020/11/25 03:19:51\n",
      "episode : 35 | final step : 99 | total reward : 39.375\n",
      "2020/11/25 03:19:52\n",
      "episode : 36 | final step : 98 | total reward : 65.69999694824219\n",
      "2020/11/25 03:19:52\n",
      "episode : 37 | final step : 98 | total reward : 31.424999237060547\n",
      "2020/11/25 03:19:53\n",
      "episode : 38 | final step : 100 | total reward : 33.400001525878906\n",
      "2020/11/25 03:19:53\n",
      "episode : 39 | final step : 99 | total reward : 30.549999237060547\n",
      "2020/11/25 03:19:54\n",
      "episode : 40 | final step : 99 | total reward : -107.625\n",
      "2020/11/25 03:19:54\n",
      "episode : 41 | final step : 102 | total reward : 10.649999618530273\n",
      "2020/11/25 03:19:55\n",
      "episode : 42 | final step : 98 | total reward : -97.0\n",
      "2020/11/25 03:19:56\n",
      "episode : 43 | final step : 98 | total reward : -71.125\n",
      "2020/11/25 03:19:56\n",
      "episode : 44 | final step : 98 | total reward : 33.0\n",
      "2020/11/25 03:19:57\n",
      "episode : 45 | final step : 97 | total reward : 59.625\n",
      "2020/11/25 03:19:57\n",
      "episode : 46 | final step : 98 | total reward : -49.20000076293945\n",
      "2020/11/25 03:19:58\n",
      "episode : 47 | final step : 100 | total reward : 48.474998474121094\n",
      "2020/11/25 03:19:58\n",
      "episode : 48 | final step : 99 | total reward : -50.95000076293945\n",
      "2020/11/25 03:19:59\n",
      "episode : 49 | final step : 98 | total reward : -63.70000076293945\n",
      "2020/11/25 03:19:59\n",
      "episode : 50 | final step : 98 | total reward : -91.1500015258789\n",
      "2020/11/25 03:19:59\n",
      "episode : 51 | final step : 98 | total reward : -41.724998474121094\n",
      "2020/11/25 03:20:00\n",
      "episode : 52 | final step : 99 | total reward : -36.974998474121094\n",
      "2020/11/25 03:20:00\n",
      "episode : 53 | final step : 98 | total reward : -69.69999694824219\n",
      "2020/11/25 03:20:01\n",
      "episode : 54 | final step : 98 | total reward : 21.274999618530273\n",
      "2020/11/25 03:20:01\n",
      "episode : 55 | final step : 98 | total reward : -58.17499923706055\n",
      "2020/11/25 03:20:02\n",
      "episode : 56 | final step : 98 | total reward : -68.0999984741211\n",
      "2020/11/25 03:20:02\n",
      "episode : 57 | final step : 98 | total reward : -63.25\n",
      "2020/11/25 03:20:03\n",
      "episode : 58 | final step : 99 | total reward : 13.949999809265137\n",
      "2020/11/25 03:20:03\n",
      "episode : 59 | final step : 98 | total reward : 45.45000076293945\n",
      "2020/11/25 03:20:04\n",
      "episode : 60 | final step : 98 | total reward : -82.8499984741211\n",
      "2020/11/25 03:20:04\n",
      "episode : 61 | final step : 98 | total reward : -39.474998474121094\n",
      "2020/11/25 03:20:05\n",
      "episode : 62 | final step : 98 | total reward : 53.849998474121094\n",
      "2020/11/25 03:20:05\n",
      "episode : 63 | final step : 99 | total reward : -48.67499923706055\n",
      "2020/11/25 03:20:06\n",
      "episode : 64 | final step : 98 | total reward : -55.474998474121094\n",
      "2020/11/25 03:20:06\n",
      "episode : 65 | final step : 100 | total reward : -4.275000095367432\n",
      "2020/11/25 03:20:07\n",
      "episode : 66 | final step : 99 | total reward : -48.32500076293945\n",
      "2020/11/25 03:20:07\n",
      "episode : 67 | final step : 97 | total reward : 46.45000076293945\n",
      "2020/11/25 03:20:07\n",
      "episode : 68 | final step : 99 | total reward : -46.82500076293945\n",
      "2020/11/25 03:20:08\n",
      "episode : 69 | final step : 96 | total reward : 44.900001525878906\n",
      "2020/11/25 03:20:08\n",
      "episode : 70 | final step : 98 | total reward : -31.549999237060547\n",
      "2020/11/25 03:20:09\n",
      "episode : 71 | final step : 98 | total reward : -68.6500015258789\n",
      "2020/11/25 03:20:09\n",
      "episode : 72 | final step : 100 | total reward : 25.450000762939453\n",
      "2020/11/25 03:20:10\n",
      "episode : 73 | final step : 99 | total reward : -90.875\n",
      "2020/11/25 03:20:10\n",
      "episode : 74 | final step : 100 | total reward : 32.04999923706055\n",
      "2020/11/25 03:20:10\n",
      "episode : 75 | final step : 101 | total reward : 18.075000762939453\n",
      "2020/11/25 03:20:11\n",
      "episode : 76 | final step : 96 | total reward : 45.70000076293945\n",
      "2020/11/25 03:20:11\n",
      "episode : 77 | final step : 98 | total reward : 42.45000076293945\n",
      "2020/11/25 03:20:12\n",
      "episode : 78 | final step : 98 | total reward : -34.82500076293945\n",
      "2020/11/25 03:20:12\n",
      "episode : 79 | final step : 98 | total reward : -75.7249984741211\n",
      "2020/11/25 03:20:13\n",
      "episode : 80 | final step : 98 | total reward : -68.92500305175781\n",
      "2020/11/25 03:20:13\n",
      "episode : 81 | final step : 102 | total reward : 21.5\n",
      "2020/11/25 03:20:14\n",
      "episode : 82 | final step : 100 | total reward : 7.599999904632568\n",
      "2020/11/25 03:20:14\n",
      "episode : 83 | final step : 100 | total reward : -80.44999694824219\n",
      "2020/11/25 03:20:15\n",
      "episode : 84 | final step : 99 | total reward : -65.875\n",
      "2020/11/25 03:20:15\n",
      "episode : 85 | final step : 99 | total reward : 39.67499923706055\n",
      "2020/11/25 03:20:16\n",
      "episode : 86 | final step : 99 | total reward : -80.9000015258789\n",
      "2020/11/25 03:20:16\n",
      "episode : 87 | final step : 99 | total reward : 32.95000076293945\n",
      "2020/11/25 03:20:17\n",
      "episode : 88 | final step : 99 | total reward : 56.0\n",
      "2020/11/25 03:20:17\n",
      "episode : 89 | final step : 102 | total reward : 22.75\n",
      "2020/11/25 03:20:18\n",
      "episode : 90 | final step : 97 | total reward : 58.70000076293945\n",
      "2020/11/25 03:20:19\n",
      "episode : 91 | final step : 98 | total reward : -70.8499984741211\n",
      "2020/11/25 03:20:19\n",
      "episode : 92 | final step : 98 | total reward : -70.125\n",
      "2020/11/25 03:20:20\n",
      "episode : 93 | final step : 98 | total reward : -94.3499984741211\n",
      "2020/11/25 03:20:20\n",
      "episode : 94 | final step : 100 | total reward : -10.774999618530273\n",
      "2020/11/25 03:20:21\n",
      "episode : 95 | final step : 99 | total reward : -86.30000305175781\n",
      "2020/11/25 03:20:21\n",
      "episode : 96 | final step : 100 | total reward : 70.25\n",
      "2020/11/25 03:20:22\n",
      "episode : 97 | final step : 98 | total reward : 28.950000762939453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/11/25 03:20:22\n",
      "episode : 98 | final step : 98 | total reward : -79.625\n",
      "2020/11/25 03:20:23\n",
      "episode : 99 | final step : 99 | total reward : -97.17500305175781\n",
      "2020/11/25 03:20:23\n",
      "episode : 100 | final step : 99 | total reward : -83.6500015258789\n",
      "2020/11/25 03:20:24\n",
      "episode : 101 | final step : 99 | total reward : -50.5\n",
      "2020/11/25 03:20:24\n",
      "episode : 102 | final step : 99 | total reward : -89.80000305175781\n",
      "2020/11/25 03:20:25\n",
      "episode : 103 | final step : 98 | total reward : -84.625\n",
      "2020/11/25 03:20:25\n",
      "episode : 104 | final step : 98 | total reward : -55.42499923706055\n",
      "2020/11/25 03:20:26\n",
      "episode : 105 | final step : 100 | total reward : 26.399999618530273\n",
      "2020/11/25 03:20:26\n",
      "episode : 106 | final step : 98 | total reward : 17.125\n",
      "2020/11/25 03:20:27\n",
      "episode : 107 | final step : 100 | total reward : -9.425000190734863\n",
      "2020/11/25 03:20:27\n",
      "episode : 108 | final step : 99 | total reward : 26.5\n",
      "2020/11/25 03:20:27\n",
      "episode : 109 | final step : 98 | total reward : -46.275001525878906\n",
      "2020/11/25 03:20:28\n",
      "episode : 110 | final step : 98 | total reward : 41.775001525878906\n",
      "2020/11/25 03:20:28\n",
      "episode : 111 | final step : 100 | total reward : 0.574999988079071\n",
      "2020/11/25 03:20:29\n",
      "episode : 112 | final step : 100 | total reward : 33.224998474121094\n",
      "2020/11/25 03:20:29\n",
      "episode : 113 | final step : 97 | total reward : 15.175000190734863\n",
      "2020/11/25 03:20:30\n",
      "episode : 114 | final step : 100 | total reward : 20.225000381469727\n",
      "2020/11/25 03:20:30\n",
      "episode : 115 | final step : 99 | total reward : -63.07500076293945\n",
      "2020/11/25 03:20:30\n",
      "episode : 116 | final step : 98 | total reward : -64.7750015258789\n",
      "2020/11/25 03:20:31\n",
      "episode : 117 | final step : 98 | total reward : -84.32499694824219\n",
      "2020/11/25 03:20:31\n",
      "episode : 118 | final step : 98 | total reward : 30.725000381469727\n",
      "2020/11/25 03:20:32\n",
      "episode : 119 | final step : 99 | total reward : -38.650001525878906\n",
      "2020/11/25 03:20:32\n",
      "episode : 120 | final step : 99 | total reward : -76.3499984741211\n",
      "2020/11/25 03:20:32\n",
      "episode : 121 | final step : 99 | total reward : 55.95000076293945\n",
      "2020/11/25 03:20:33\n",
      "episode : 122 | final step : 98 | total reward : -60.775001525878906\n",
      "2020/11/25 03:20:33\n",
      "episode : 123 | final step : 98 | total reward : -60.849998474121094\n",
      "2020/11/25 03:20:34\n",
      "episode : 124 | final step : 97 | total reward : 35.79999923706055\n",
      "2020/11/25 03:20:34\n",
      "episode : 125 | final step : 98 | total reward : 18.100000381469727\n",
      "2020/11/25 03:20:35\n",
      "episode : 126 | final step : 100 | total reward : 13.449999809265137\n",
      "2020/11/25 03:20:35\n",
      "episode : 127 | final step : 99 | total reward : 10.824999809265137\n",
      "2020/11/25 03:20:36\n",
      "episode : 128 | final step : 99 | total reward : -85.55000305175781\n",
      "2020/11/25 03:20:36\n",
      "episode : 129 | final step : 99 | total reward : 3.200000047683716\n",
      "2020/11/25 03:20:37\n",
      "episode : 130 | final step : 98 | total reward : -81.07499694824219\n",
      "2020/11/25 03:20:37\n",
      "episode : 131 | final step : 99 | total reward : -51.224998474121094\n",
      "2020/11/25 03:20:38\n",
      "episode : 132 | final step : 98 | total reward : -74.75\n",
      "2020/11/25 03:20:38\n",
      "episode : 133 | final step : 99 | total reward : 34.525001525878906\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10000\n",
    "total_res=[]\n",
    "reward_list=[]\n",
    "for i_episode in range(num_episodes):\n",
    "    total_reward=0\n",
    "    \n",
    "    # 환경과 상태 초기화\n",
    "    res_list=np.zeros(11)\n",
    "    state = env.reset()\n",
    "    state=torch.from_numpy(state.astype(np.float32)).unsqueeze(0).to(device)\n",
    "    for t in count():\n",
    "        # 행동 선택과 수행\n",
    "        \n",
    "        action = select_action(state)\n",
    "        next_state, reward, done, res = env.step(action.item())\n",
    "        reward = torch.tensor([reward], dtype=torch.float32).to(device)\n",
    "        \n",
    "        next_state=torch.from_numpy(next_state.astype(np.float32)).unsqueeze(0).to(device)\n",
    "\n",
    "        # 새로운 상태 관찰\n",
    "        if not done:\n",
    "            next_state = next_state\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # 메모리에 변이 저장\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # 다음 상태로 이동\n",
    "        state = next_state\n",
    "\n",
    "        # 최적화 한단계 수행(목표 네트워크에서)\n",
    "        optimize_model()\n",
    "        \n",
    "        # Data save\n",
    "        \n",
    "        cmd_list,r_list,elev_list,azim_list,Pm_list,Pt_list,h_list=res\n",
    "        Pm_list=Pm_list.tolist()\n",
    "        Pt_list=Pt_list.tolist()\n",
    "        merged_data=itertools.chain([cmd_list],[r_list],[elev_list],[azim_list],Pm_list,Pt_list,[h_list])\n",
    "        merged_data=np.array(list(merged_data))\n",
    "        res_list=np.vstack([res_list,merged_data])\n",
    "        \n",
    "        total_reward+=reward\n",
    "        \n",
    "        if done:\n",
    "            res_list=np.delete(res_list,0,0)\n",
    "            \n",
    "            total_res.append(res_list)\n",
    "            reward_list.append(total_reward)\n",
    "            \n",
    "            now = time.localtime()\n",
    "            print (\"%04d/%02d/%02d %02d:%02d:%02d\" % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec))\n",
    "            print(\"episode : {} | final step : {} | total reward : {}\".format(i_episode, t, total_reward.item()))\n",
    "            break\n",
    "            \n",
    "        \n",
    "    #목표 네트워크 업데이트, 모든 웨이트와 바이어스 복사\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total rewards\")\n",
    "plt.plot(reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots\n",
    "Deg2Rad = np.pi/180\n",
    "Rad2Deg = 1/Deg2Rad\n",
    "\n",
    "plt_res=total_res[9999]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,9), dpi=100)\n",
    "\n",
    "plt.subplot(511)\n",
    "plt.plot(plt_res[:,0], label=r'$\\dot{h}_{cmd}$')\n",
    "plt.ylabel(r'$\\dot{h}_{cmd}$ ($m/s$)'), plt.grid()\n",
    "\n",
    "plt.subplot(512)\n",
    "plt.plot(plt_res[:,10],label=r'$\\{h}$')\n",
    "plt.ylabel(r'$h$ (m)'), plt.grid()\n",
    "\n",
    "plt.subplot(513)\n",
    "plt.plot(plt_res[:,1],label=r'$\\{r}$')\n",
    "plt.ylabel(r'$r$ (m)'), plt.grid()\n",
    "\n",
    "plt.subplot(514)\n",
    "plt.plot(plt_res[:,2]*Rad2Deg, label='elevation')\n",
    "plt.ylabel('elevation (deg)'), plt.grid()\n",
    "\n",
    "plt.subplot(515)\n",
    "plt.plot(plt_res[:,3]*Rad2Deg, label='azimuth')\n",
    "plt.ylabel('azimuth (deg)'), plt.grid()\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trajectory plots\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.figure(figsize=(12,9), dpi=100)\n",
    "plt.gca(projection='3d')\n",
    "plt.plot(plt_res[:,5], plt_res[:,4], -plt_res[:,6], label='player', linewidth=3)\n",
    "plt.plot(plt_res[:,8], plt_res[:,7], -plt_res[:,9], label='target', linewidth=3)\n",
    "plt.xlabel('East')\n",
    "plt.ylabel('North')\n",
    "plt.xlim(-2000,2000)\n",
    "plt.ylim(0,4000)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,9), dpi=100)\n",
    "plt.plot(plt_res[:,5], plt_res[:,4], label='player', linewidth=3)\n",
    "plt.plot(plt_res[:,8], plt_res[:,7], label='target', linewidth=3)\n",
    "plt.xlabel('East')\n",
    "plt.ylabel('North')\n",
    "plt.grid(), plt.legend(), plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,9), dpi=100)\n",
    "plt.plot(plt_res[:,4], -plt_res[:,6], label='player', linewidth=3)\n",
    "plt.plot(plt_res[:,7], -plt_res[:,9], label='target', linewidth=3)\n",
    "plt.xlabel('North')\n",
    "plt.ylabel('Up')\n",
    "plt.grid(), plt.legend(), plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
