{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:23:33.065767Z",
     "start_time": "2021-01-26T07:23:32.363150Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.distributions import Categorical\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "import gym\n",
    "import gym_Aircraft\n",
    "import seaborn as sns\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:23:33.071309Z",
     "start_time": "2021-01-26T07:23:33.067881Z"
    }
   },
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:23:33.122796Z",
     "start_time": "2021-01-26T07:23:33.072763Z"
    }
   },
   "outputs": [],
   "source": [
    "class FClayer(nn.Module):\n",
    "    def __init__(self, innodes: int, nodes: int):\n",
    "        super(FClayer, self).__init__()\n",
    "        self.fc=nn.Linear(innodes,nodes)\n",
    "        self.act=nn.LeakyReLU(0.2, inplace=True)\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        out=self.fc(x)\n",
    "        out=self.act(out)\n",
    "        return out\n",
    "class WaveNET(nn.Module):\n",
    "    def __init__(self, block: Type[Union[FClayer]], planes: List[int], nodes: List[int], num_classes: int = 3\n",
    "                ) -> None:\n",
    "        super(WaveNET, self).__init__()\n",
    "        self.innodes=5\n",
    "        \n",
    "        self.layer1=self._make_layer(block, planes[0], nodes[0])\n",
    "        self.layer2=self._make_layer(block, planes[1], nodes[1])\n",
    "        self.layer3=self._make_layer(block, planes[2], nodes[2])\n",
    "        \n",
    "        self.fin_fc=nn.Linear(self.innodes,num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "    \n",
    "    def _make_layer(self, block: Type[Union[FClayer]], planes: int, nodes: int) -> nn.Sequential:\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.innodes, nodes))\n",
    "        self.innodes = nodes\n",
    "        for _ in range(1, planes):\n",
    "            layers.append(block(self.innodes, nodes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "        \n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.fin_fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, actor_model, critic_model):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        # actor\n",
    "        self.action_layer = actor_model\n",
    "        \n",
    "        # critic\n",
    "        self.value_layer = critic_model\n",
    "        \n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def act(self, state, memory):\n",
    "        state = torch.from_numpy(state).float().to(device) \n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(action)\n",
    "        memory.logprobs.append(dist.log_prob(action))\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def evaluate(self, state, action):\n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        \n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        \n",
    "        state_value = self.value_layer(state)\n",
    "        \n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, actor_model, critic_model, lr, betas, gamma, K_epochs, eps_clip):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.policy = ActorCritic(actor_model, critic_model).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
    "        self.policy_old = ActorCritic(actor_model, critic_model).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "    \n",
    "    def update(self, memory):   \n",
    "        # Monte Carlo estimate of state rewards:\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "        \n",
    "        # Normalizing the rewards:\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "        \n",
    "        # convert list to tensor\n",
    "        old_states = torch.stack(memory.states).to(device).detach()\n",
    "        old_actions = torch.stack(memory.actions).to(device).detach()\n",
    "        old_logprobs = torch.stack(memory.logprobs).to(device).detach()\n",
    "        \n",
    "        # Optimize policy for K epochs:\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluating old actions and values :\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "            \n",
    "            # Finding the ratio (pi_theta / pi_theta__old):\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "                \n",
    "            # Finding Surrogate Loss:\n",
    "            advantages = rewards - state_values.detach()\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "            \n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        # Copy new weights into old policy:\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1000\n",
    "succeed_coef = 7000         # maximum reward when agent avoids collision\n",
    "collide_coef = -1000        # reward when agent doesn't avoid collision\n",
    "change_cmd_penalty = -100   # reward when agent changes command values\n",
    "cmd_penalty = -0.05         # coefficient of penaly on using command\n",
    "lr = 0.002\n",
    "betas = (0.9, 0.999)\n",
    "gamma = 0.99                # discount factor\n",
    "K_epochs = 4                # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "max_timesteps = 300\n",
    "env_name = \"acav-v0\"\n",
    "env = gym.make(env_name)\n",
    "env.env.__init__(succeed_coef, collide_coef, change_cmd_penalty, cmd_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-kunwoopark/.local/lib/python3.7/site-packages/torch/serialization.py:625: UserWarning: Couldn't retrieve source code for container of type WaveNET. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/home/jupyter-kunwoopark/.local/lib/python3.7/site-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/jupyter-kunwoopark/.local/lib/python3.7/site-packages/torch/serialization.py:625: UserWarning: Couldn't retrieve source code for container of type FClayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/home/jupyter-kunwoopark/.local/lib/python3.7/site-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/jupyter-kunwoopark/.local/lib/python3.7/site-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.activation.LeakyReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "actor_model = torch.load(\"./Custom_model_fin\")\n",
    "critic_model = torch.load(\"./Custom_model_fin\")\n",
    "mean = np.load('mean_test.npy')\n",
    "std = np.load('std_test.npy')\n",
    "num_final_nodes = critic_model.fin_fc.in_features\n",
    "critic_model.fin_fc = nn.Linear(num_final_nodes, 1)\n",
    "num_final_nodes = actor_model.fin_fc.in_features\n",
    "num_output_nodes = actor_model.fin_fc.out_features\n",
    "actor_model.fin_fc = nn.Sequential(nn.Linear(num_final_nodes, num_output_nodes), nn.Softmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:23:35.730259Z",
     "start_time": "2021-01-26T07:23:33.126156Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = Memory()\n",
    "ppo = PPO(actor_model, critic_model, lr, betas, gamma, K_epochs, eps_clip)\n",
    "ppo.policy_old.load_state_dict(torch.load(\"PPO_1.28.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_res=[]\n",
    "rewards   = []\n",
    "for ep in range(1, n_episodes+1):\n",
    "    res_list = np.zeros(11)\n",
    "    ep_reward = 0\n",
    "    state = env.reset()\n",
    "    for t in range(max_timesteps):\n",
    "        state = (state-mean)/std\n",
    "        action = ppo.policy_old.act(state, memory)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        \n",
    "        cmd_list,r_list,elev_list,azim_list,Pm_list,Pt_list,h_list,height_diff=info[\"info\"]\n",
    "        Pm_list=Pm_list.tolist()\n",
    "        Pt_list=Pt_list.tolist()\n",
    "        merged_data=itertools.chain([cmd_list],[r_list],[elev_list],[azim_list],Pm_list,Pt_list,[h_list])\n",
    "        merged_data=np.array(list(merged_data))\n",
    "        res_list=np.vstack([res_list,merged_data])\n",
    "        \n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            res_list=np.delete(res_list,0,0)\n",
    "            total_res.append(res_list)\n",
    "            rewards.append(ep_reward)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_distance = []\n",
    "for i in range (n_episodes):\n",
    "    min_distance.append(min(total_res[i][:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(min_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "sns.set(color_codes=True)\n",
    "sns.distplot(min_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deg2Rad = np.pi/180\n",
    "Rad2Deg = 1/Deg2Rad\n",
    "\n",
    "plt_res=total_res[10]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,9), dpi=100)\n",
    "\n",
    "plt.subplot(511)\n",
    "plt.plot(plt_res[:,0], label=r'$\\dot{h}_{cmd}$')\n",
    "plt.ylabel(r'$\\dot{h}_{cmd}$ ($m/s$)'), plt.grid()\n",
    "\n",
    "plt.subplot(512)\n",
    "plt.plot(plt_res[:,10],label=r'$\\{h}$')\n",
    "plt.ylabel(r'$h$ (m)'), plt.grid()\n",
    "\n",
    "plt.subplot(513)\n",
    "plt.plot(plt_res[:,1],label=r'$\\{r}$')\n",
    "plt.ylabel(r'$r$ (m)'), plt.grid()\n",
    "\n",
    "plt.subplot(514)\n",
    "plt.plot(plt_res[:,2]*Rad2Deg, label='elevation')\n",
    "plt.ylabel('elevation (deg)'), plt.grid()\n",
    "\n",
    "plt.subplot(515)\n",
    "plt.plot(plt_res[:,3]*Rad2Deg, label='azimuth')\n",
    "plt.ylabel('azimuth (deg)'), plt.grid()\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
