{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import time\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "import gym_Aircraft\n",
    "from torch import Tensor\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating environment\n",
    "succeed_coef = 8000         # maximum reward when agent avoids collision\n",
    "collide_coef = -4000        # reward when agent doesn't avoid collision\n",
    "change_cmd_penalty = -100   # reward when agent changes command values\n",
    "cmd_penalty = -0.15          # coefficient of penaly on using command\n",
    "cmd_suit_coef = -100         # coefficient of suitable command\n",
    "start_cond_coef = 100       # coefficient of condition on begining\n",
    "step_size = 10000          # lr scheduling step size\n",
    "\n",
    "lr = 0.0001\n",
    "betas = (0.9, 0.999)\n",
    "\n",
    "env_name = \"acav-v0\"\n",
    "env = gym.make(env_name)\n",
    "env.env.__init__(succeed_coef, collide_coef, change_cmd_penalty, cmd_penalty, start_cond_coef, cmd_suit_coef)\n",
    "render = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"transition 저장\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FClayer(nn.Module):\n",
    "    def __init__(self, innodes: int, nodes: int):\n",
    "        super(FClayer, self).__init__()\n",
    "        self.fc=nn.Linear(innodes,nodes)\n",
    "        self.act=nn.LeakyReLU(0.2, inplace=True)\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        out=self.fc(x)\n",
    "        out=self.act(out)\n",
    "        return out\n",
    "    \n",
    "class WaveNET(nn.Module):\n",
    "    def __init__(self, block: Type[Union[FClayer]], planes: List[int], nodes: List[int], num_classes: int = 3\n",
    "                ) -> None:\n",
    "        super(WaveNET, self).__init__()\n",
    "        self.innodes=5\n",
    "        \n",
    "        self.layer1=self._make_layer(block, planes[0], nodes[0])\n",
    "        self.layer2=self._make_layer(block, planes[1], nodes[1])\n",
    "        self.layer3=self._make_layer(block, planes[2], nodes[2])\n",
    "        \n",
    "        self.fin_fc=nn.Linear(self.innodes,num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "    \n",
    "    def _make_layer(self, block: Type[Union[FClayer]], planes: int, nodes: int) -> nn.Sequential:\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.innodes, nodes))\n",
    "        self.innodes = nodes\n",
    "        for _ in range(1, planes):\n",
    "            layers.append(block(self.innodes, nodes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "        \n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.fin_fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-kunwoopark/.local/lib/python3.7/site-packages/torch/serialization.py:625: UserWarning: Couldn't retrieve source code for container of type WaveNET. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/home/jupyter-kunwoopark/.local/lib/python3.7/site-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/jupyter-kunwoopark/.local/lib/python3.7/site-packages/torch/serialization.py:625: UserWarning: Couldn't retrieve source code for container of type FClayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/home/jupyter-kunwoopark/.local/lib/python3.7/site-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/jupyter-kunwoopark/.local/lib/python3.7/site-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.activation.LeakyReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "policy_net = torch.load(\"./Custom_model_fin\").to(device)\n",
    "target_net = torch.load(\"./Custom_model_fin\").to(device)\n",
    "target_net.eval()\n",
    "mean=np.load('mean_test.npy')\n",
    "std=np.load('std_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WaveNET(\n",
       "  (layer1): Sequential(\n",
       "    (0): FClayer(\n",
       "      (fc): Linear(in_features=5, out_features=40, bias=True)\n",
       "      (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (1): FClayer(\n",
       "      (fc): Linear(in_features=40, out_features=40, bias=True)\n",
       "      (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): FClayer(\n",
       "      (fc): Linear(in_features=40, out_features=20, bias=True)\n",
       "      (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (1): FClayer(\n",
       "      (fc): Linear(in_features=20, out_features=20, bias=True)\n",
       "      (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): FClayer(\n",
       "      (fc): Linear(in_features=20, out_features=60, bias=True)\n",
       "      (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (1): FClayer(\n",
       "      (fc): Linear(in_features=60, out_features=60, bias=True)\n",
       "      (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (fin_fc): Linear(in_features=60, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net.apply(init_weights)\n",
    "target_net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy_net.parameters(), lr=lr, betas=betas)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = step_size)\n",
    "memory = ReplayMemory(200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max (1)은 각 행의 가장 큰 열 값을 반환합니다.\n",
    "            # 최대 결과의 두번째 열은 최대 요소의 주소값이므로,\n",
    "            # 기대 보상이 더 큰 행동을 선택할 수 있습니다.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "#         return policy_net(state).max(1)[1].view(1, 1)\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # 기대 Q 값 계산\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Huber 손실 계산\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # 모델 최적화\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "env.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-kunwoopark/.local/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021/03/12 19:19:00\n",
      "episode : 0 | final step : 95 | total reward : -9122.0\n",
      "2021/03/12 19:19:34\n",
      "episode : 50 | final step : 98 | total reward : -6185.0\n",
      "2021/03/12 19:20:08\n",
      "episode : 100 | final step : 99 | total reward : 7207.0\n",
      "2021/03/12 19:20:41\n",
      "episode : 150 | final step : 98 | total reward : -4000.0\n",
      "2021/03/12 19:21:16\n",
      "episode : 200 | final step : 99 | total reward : -5255.0\n",
      "2021/03/12 19:21:49\n",
      "episode : 250 | final step : 101 | total reward : 7218.0\n",
      "2021/03/12 19:22:22\n",
      "episode : 300 | final step : 98 | total reward : -4971.0\n",
      "2021/03/12 19:22:55\n",
      "episode : 350 | final step : 98 | total reward : -5973.0\n",
      "2021/03/12 19:23:28\n",
      "episode : 400 | final step : 98 | total reward : -5027.0\n",
      "2021/03/12 19:24:02\n",
      "episode : 450 | final step : 98 | total reward : -4306.0\n",
      "2021/03/12 19:24:36\n",
      "episode : 500 | final step : 98 | total reward : -6549.0\n",
      "2021/03/12 19:25:09\n",
      "episode : 550 | final step : 98 | total reward : -8391.0\n",
      "2021/03/12 19:25:41\n",
      "episode : 600 | final step : 98 | total reward : -5238.0\n",
      "2021/03/12 19:26:13\n",
      "episode : 650 | final step : 98 | total reward : -8186.0\n",
      "2021/03/12 19:26:47\n",
      "episode : 700 | final step : 100 | total reward : -2631.0\n",
      "2021/03/12 19:27:21\n",
      "episode : 750 | final step : 98 | total reward : -6531.0\n",
      "2021/03/12 19:27:55\n",
      "episode : 800 | final step : 98 | total reward : -8396.0\n",
      "2021/03/12 19:28:29\n",
      "episode : 850 | final step : 100 | total reward : 3415.0\n",
      "2021/03/12 19:29:02\n",
      "episode : 900 | final step : 99 | total reward : -4650.0\n",
      "2021/03/12 19:29:35\n",
      "episode : 950 | final step : 98 | total reward : -7013.0\n",
      "2021/03/12 19:30:09\n",
      "episode : 1000 | final step : 99 | total reward : -5855.0\n",
      "2021/03/12 19:30:43\n",
      "episode : 1050 | final step : 97 | total reward : -16252.0\n",
      "2021/03/12 19:31:15\n",
      "episode : 1100 | final step : 99 | total reward : -15387.0\n",
      "2021/03/12 19:31:51\n",
      "episode : 1150 | final step : 100 | total reward : -11561.0\n",
      "2021/03/12 19:32:23\n",
      "episode : 1200 | final step : 101 | total reward : -16368.0\n",
      "2021/03/12 19:32:55\n",
      "episode : 1250 | final step : 99 | total reward : -5674.0\n",
      "2021/03/12 19:33:29\n",
      "episode : 1300 | final step : 98 | total reward : -17313.0\n",
      "2021/03/12 19:34:05\n",
      "episode : 1350 | final step : 98 | total reward : -9903.0\n",
      "2021/03/12 19:34:37\n",
      "episode : 1400 | final step : 98 | total reward : -10325.0\n",
      "2021/03/12 19:35:10\n",
      "episode : 1450 | final step : 99 | total reward : -10170.0\n",
      "2021/03/12 19:35:43\n",
      "episode : 1500 | final step : 99 | total reward : -16828.0\n",
      "2021/03/12 19:36:16\n",
      "episode : 1550 | final step : 100 | total reward : -14606.0\n",
      "2021/03/12 19:36:47\n",
      "episode : 1600 | final step : 99 | total reward : -17077.0\n",
      "2021/03/12 19:37:24\n",
      "episode : 1650 | final step : 99 | total reward : -5575.0\n",
      "2021/03/12 19:37:53\n",
      "episode : 1700 | final step : 99 | total reward : 6331.0\n",
      "2021/03/12 19:38:23\n",
      "episode : 1750 | final step : 94 | total reward : -14225.0\n",
      "2021/03/12 19:38:56\n",
      "episode : 1800 | final step : 98 | total reward : -6749.0\n",
      "2021/03/12 19:39:27\n",
      "episode : 1850 | final step : 98 | total reward : -6380.0\n",
      "2021/03/12 19:39:59\n",
      "episode : 1900 | final step : 98 | total reward : -7665.0\n",
      "2021/03/12 19:40:31\n",
      "episode : 1950 | final step : 98 | total reward : -7396.0\n",
      "2021/03/12 19:41:03\n",
      "episode : 2000 | final step : 96 | total reward : -15865.0\n",
      "2021/03/12 19:41:36\n",
      "episode : 2050 | final step : 99 | total reward : -4814.0\n",
      "2021/03/12 19:42:09\n",
      "episode : 2100 | final step : 98 | total reward : -16533.0\n",
      "2021/03/12 19:42:38\n",
      "episode : 2150 | final step : 99 | total reward : 5605.0\n",
      "2021/03/12 19:43:11\n",
      "episode : 2200 | final step : 99 | total reward : -16207.0\n",
      "2021/03/12 19:43:40\n",
      "episode : 2250 | final step : 98 | total reward : 6231.0\n",
      "2021/03/12 19:44:14\n",
      "episode : 2300 | final step : 92 | total reward : -5819.0\n",
      "2021/03/12 19:44:45\n",
      "episode : 2350 | final step : 98 | total reward : -5860.0\n",
      "2021/03/12 19:45:19\n",
      "episode : 2400 | final step : 99 | total reward : -7444.0\n",
      "2021/03/12 19:45:52\n",
      "episode : 2450 | final step : 98 | total reward : -4000.0\n",
      "2021/03/12 19:46:25\n",
      "episode : 2500 | final step : 98 | total reward : -7001.0\n",
      "2021/03/12 19:46:58\n",
      "episode : 2550 | final step : 98 | total reward : -5280.0\n",
      "2021/03/12 19:47:29\n",
      "episode : 2600 | final step : 100 | total reward : -7725.0\n",
      "2021/03/12 19:48:02\n",
      "episode : 2650 | final step : 98 | total reward : -15767.0\n",
      "2021/03/12 19:48:35\n",
      "episode : 2700 | final step : 96 | total reward : -6365.0\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 50000\n",
    "total_res=[]\n",
    "reward_list=[]\n",
    "for i_episode in range(num_episodes):\n",
    "    total_reward=0\n",
    "    \n",
    "    # 환경과 상태 초기화\n",
    "    res_list=np.zeros(11)\n",
    "    state = env.reset()\n",
    "    state = (state-mean)/std\n",
    "    state=torch.from_numpy(state.astype(np.float32)).unsqueeze(0).to(device)\n",
    "    for t in count():\n",
    "        # 행동 선택과 수행\n",
    "        \n",
    "        action = select_action(state)\n",
    "        next_state, reward, done, info = env.step(action.item())\n",
    "        reward = torch.tensor([reward], dtype=torch.float32).to(device)\n",
    "        \n",
    "        next_state = (next_state-mean)/std\n",
    "        next_state=torch.from_numpy(next_state.astype(np.float32)).unsqueeze(0).to(device)\n",
    "\n",
    "        # 새로운 상태 관찰\n",
    "        if not done:\n",
    "            next_state = next_state\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # 메모리에 변이 저장\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # 다음 상태로 이동\n",
    "        state = next_state\n",
    "\n",
    "        # 최적화 한단계 수행(목표 네트워크에서)\n",
    "        optimize_model()\n",
    "        \n",
    "        # Data save\n",
    "        \n",
    "        cmd_list,r_list,elev_list,azim_list,Pm_list,Pt_list,h_list, height_diff_list = info[\"info\"]\n",
    "        Pm_list=Pm_list.tolist()\n",
    "        Pt_list=Pt_list.tolist()\n",
    "        merged_data=itertools.chain([cmd_list],[r_list],[elev_list],[azim_list],Pm_list,Pt_list,[h_list])\n",
    "        merged_data=np.array(list(merged_data))\n",
    "        res_list=np.vstack([res_list,merged_data])\n",
    "        \n",
    "        total_reward+=reward\n",
    "        \n",
    "        if done:\n",
    "            res_list=np.delete(res_list,0,0)\n",
    "            \n",
    "            total_res.append(res_list)\n",
    "            reward_list.append(total_reward)\n",
    "            \n",
    "            break\n",
    "            \n",
    "    if i_episode % 50 == 0:\n",
    "        now = time.localtime()\n",
    "        print (\"%04d/%02d/%02d %02d:%02d:%02d\" % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec))\n",
    "        print(\"episode : {} | final step : {} | total reward : {}\".format(i_episode, t, total_reward.item()))\n",
    "            \n",
    "        \n",
    "    #목표 네트워크 업데이트, 모든 웨이트와 바이어스 복사\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "print('Complete')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_average(data,n):\n",
    "    res = [np.mean(data[i*n:(i+1)*n]) for i in range (int((len(data)/n)))]\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(reward_list)):\n",
    "    reward_list[i]=reward_list[i].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_number=50\n",
    "filtered_data=step_average(reward_list,average_number)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total rewards\")\n",
    "plt.plot(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"DQN_non_model_reward_large_buff.csv\", np.array(filtered_data), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots\n",
    "Deg2Rad = np.pi/180\n",
    "Rad2Deg = 1/Deg2Rad\n",
    "\n",
    "plt_res=total_res[-3]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,9), dpi=100)\n",
    "\n",
    "plt.subplot(511)\n",
    "plt.plot(plt_res[:,0], label=r'$\\dot{h}_{cmd}$')\n",
    "plt.ylabel(r'$\\dot{h}_{cmd}$ ($m/s$)'), plt.grid()\n",
    "\n",
    "plt.subplot(512)\n",
    "plt.plot(plt_res[:,10],label=r'$\\{h}$')\n",
    "plt.ylabel(r'$h$ (m)'), plt.grid()\n",
    "\n",
    "plt.subplot(513)\n",
    "plt.plot(plt_res[:,1],label=r'$\\{r}$')\n",
    "plt.ylabel(r'$r$ (m)'), plt.grid()\n",
    "\n",
    "plt.subplot(514)\n",
    "plt.plot(plt_res[:,2]*Rad2Deg, label='elevation')\n",
    "plt.ylabel('elevation (deg)'), plt.grid()\n",
    "\n",
    "plt.subplot(515)\n",
    "plt.plot(plt_res[:,3]*Rad2Deg, label='azimuth')\n",
    "plt.ylabel('azimuth (deg)'), plt.grid()\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trajectory plots\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.figure(figsize=(12,9), dpi=100)\n",
    "plt.gca(projection='3d')\n",
    "plt.plot(plt_res[:,5], plt_res[:,4], -plt_res[:,6], label='player', linewidth=3)\n",
    "plt.plot(plt_res[:,8], plt_res[:,7], -plt_res[:,9], label='target', linewidth=3)\n",
    "plt.xlabel('East')\n",
    "plt.ylabel('North')\n",
    "plt.xlim(-2000,2000)\n",
    "plt.ylim(0,4000)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,9), dpi=100)\n",
    "plt.plot(plt_res[:,5], plt_res[:,4], label='player', linewidth=3)\n",
    "plt.plot(plt_res[:,8], plt_res[:,7], label='target', linewidth=3)\n",
    "plt.xlabel('East')\n",
    "plt.ylabel('North')\n",
    "plt.grid(), plt.legend(), plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,9), dpi=100)\n",
    "plt.plot(plt_res[:,4], -plt_res[:,6], label='player', linewidth=3)\n",
    "plt.plot(plt_res[:,7], -plt_res[:,9], label='target', linewidth=3)\n",
    "plt.xlabel('North')\n",
    "plt.ylabel('Up')\n",
    "plt.grid(), plt.legend(), plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
